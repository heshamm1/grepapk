#!/usr/bin/env python3
"""
AI-Powered Vulnerability Detection Module for GrepAPK
Integrates CodeBERT/CodeT5 for intelligent code analysis and vulnerability detection.
"""

import os
import re
import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import numpy as np
from dataclasses import dataclass

try:
    import torch
    from transformers import (
        AutoTokenizer, 
        AutoModel, 
        AutoModelForSequenceClassification,
        pipeline
    )
    from sklearn.metrics.pairwise import cosine_similarity
    AI_AVAILABLE = True
except ImportError:
    AI_AVAILABLE = False
    # Only log warning when actually trying to use AI features
    # logging.warning("AI dependencies not available. Install torch and transformers for AI features.")

logger = logging.getLogger(__name__)

@dataclass
class AIVulnerabilityAssessment:
    """AI-powered vulnerability assessment result."""
    vulnerability_type: str
    confidence_score: float
    severity_level: str
    context_analysis: str
    false_positive_probability: float
    exploitation_difficulty: str
    remediation_suggestions: List[str]
    code_context: str
    line_numbers: List[int]

class CodeBERTVulnerabilityDetector:
    """CodeBERT-based vulnerability detector for Android applications."""
    
    def __init__(self, model_name: str = "microsoft/codebert-base", device: str = "auto"):
        """
        Initialize the CodeBERT vulnerability detector.
        
        Args:
            model_name: Pre-trained model to use
            device: Device to run inference on ('cpu', 'cuda', or 'auto')
        """
        if not AI_AVAILABLE:
            raise ImportError("AI dependencies not available. Install torch and transformers.")
        
        self.model_name = model_name
        self.device = self._setup_device(device)
        
        # Initialize models and tokenizers
        self._load_models()
        
        # Vulnerability patterns for AI analysis
        self.vulnerability_patterns = self._load_vulnerability_patterns()
        
        # Context window size for analysis
        self.context_window = 512
        
        logger.info(f"AI Vulnerability Detector initialized with {model_name} on {self.device}")
    
    def _setup_device(self, device: str) -> str:
        """Setup the device for model inference."""
        if device == "auto":
            if torch.cuda.is_available():
                device = "cuda"
                logger.info("CUDA available, using GPU acceleration")
            else:
                device = "cpu"
                logger.info("CUDA not available, using CPU")
        return device
    
    def _load_models(self):
        """Load CodeBERT models and tokenizers."""
        try:
            # Load base CodeBERT model for code understanding
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModel.from_pretrained(self.model_name)
            
            # Load vulnerability classification model (fine-tuned if available)
            try:
                self.classifier = AutoModelForSequenceClassification.from_pretrained(
                    "microsoft/codebert-base", 
                    num_labels=len(self.vulnerability_patterns)
                )
            except:
                # Fallback to base model if classifier not available
                self.classifier = None
                logger.info("Vulnerability classifier not available, using base model")
            
            # Move models to device
            self.model.to(self.device)
            if self.classifier:
                self.classifier.to(self.device)
            
            # Set models to evaluation mode
            self.model.eval()
            if self.classifier:
                self.classifier.eval()
                
        except Exception as e:
            logger.error(f"Failed to load AI models: {e}")
            raise
    
    def _load_vulnerability_patterns(self) -> Dict[str, Dict[str, Any]]:
        """Load vulnerability patterns for AI analysis."""
        return {
            'sql_injection': {
                'keywords': ['rawQuery', 'execSQL', 'query', 'SELECT', 'INSERT', 'UPDATE', 'DELETE'],
                'context_patterns': [r'\+.*user.*input', r'\+.*variable', r'\+.*parameter'],
                'severity': 'HIGH',
                'description': 'SQL injection vulnerability detected through code analysis'
            },
            'command_injection': {
                'keywords': ['Runtime.exec', 'ProcessBuilder', 'shell', 'command'],
                'context_patterns': [r'\+.*user.*input', r'\+.*variable'],
                'severity': 'HIGH',
                'description': 'Command injection vulnerability detected through code analysis'
            },
            'path_traversal': {
                'keywords': ['File', 'FileInputStream', 'openFileInput', 'getSharedPreferences'],
                'context_patterns': [r'\+.*\.\.', r'\+.*user.*input', r'\+.*variable'],
                'severity': 'MEDIUM',
                'description': 'Path traversal vulnerability detected through code analysis'
            },
            'xss': {
                'keywords': ['WebView', 'loadUrl', 'javascript:', 'setJavaScriptEnabled'],
                'context_patterns': [r'\+.*user.*input', r'\+.*variable', r'<script>'],
                'severity': 'MEDIUM',
                'description': 'Cross-site scripting vulnerability detected through code analysis'
            },
            'hardcoded_secrets': {
                'keywords': ['password', 'secret', 'key', 'token', 'credential'],
                'context_patterns': [r'=.*["\'][^"\']{8,}["\']', r'const.*["\'][^"\']{8,}["\']'],
                'severity': 'HIGH',
                'description': 'Hardcoded secrets detected through code analysis'
            },
            'insecure_components': {
                'keywords': ['exported', 'permission', 'intent-filter', 'android:exported'],
                'context_patterns': [r'exported.*true', r'permission.*android:permission'],
                'severity': 'MEDIUM',
                'description': 'Insecure component configuration detected through code analysis'
            }
        }
    
    def analyze_code_file(self, file_path: Path, content: str) -> List[AIVulnerabilityAssessment]:
        """
        Analyze a code file for vulnerabilities using AI.
        
        Args:
            file_path: Path to the file being analyzed
            content: File content as string
            
        Returns:
            List of AI vulnerability assessments
        """
        if not AI_AVAILABLE:
            return []
        
        assessments = []
        
        try:
            # Split content into manageable chunks
            code_chunks = self._split_code_into_chunks(content)
            
            for chunk_idx, chunk in enumerate(code_chunks):
                # Analyze each chunk
                chunk_assessments = self._analyze_code_chunk(chunk, chunk_idx, file_path)
                assessments.extend(chunk_assessments)
            
            # Merge overlapping assessments
            assessments = self._merge_overlapping_assessments(assessments)
            
            # Filter by confidence threshold
            assessments = [a for a in assessments if a.confidence_score > 0.6]
            
        except Exception as e:
            logger.error(f"Error analyzing file {file_path}: {e}")
        
        return assessments
    
    def _split_code_into_chunks(self, content: str) -> List[str]:
        """Split code content into manageable chunks for AI analysis."""
        lines = content.split('\n')
        chunks = []
        current_chunk = []
        current_length = 0
        
        for line in lines:
            if current_length + len(line) > self.context_window:
                if current_chunk:
                    chunks.append('\n'.join(current_chunk))
                current_chunk = [line]
                current_length = len(line)
            else:
                current_chunk.append(line)
                current_length += len(line)
        
        if current_chunk:
            chunks.append('\n'.join(current_chunk))
        
        return chunks
    
    def _analyze_code_chunk(self, chunk: str, chunk_idx: int, file_path: Path) -> List[AIVulnerabilityAssessment]:
        """Analyze a single code chunk for vulnerabilities."""
        assessments = []
        
        try:
            # Get code embeddings
            embeddings = self._get_code_embeddings(chunk)
            
            # Analyze for each vulnerability type
            for vuln_type, pattern_info in self.vulnerability_patterns.items():
                assessment = self._assess_vulnerability_type(
                    chunk, chunk_idx, file_path, vuln_type, pattern_info, embeddings
                )
                if assessment:
                    assessments.append(assessment)
        
        except Exception as e:
            logger.error(f"Error analyzing code chunk: {e}")
        
        return assessments
    
    def _get_code_embeddings(self, code: str) -> np.ndarray:
        """Get CodeBERT embeddings for the given code."""
        try:
            # Tokenize code
            inputs = self.tokenizer(
                code, 
                return_tensors="pt", 
                truncation=True, 
                max_length=self.context_window,
                padding=True
            )
            
            # Move to device
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # Get embeddings
            with torch.no_grad():
                outputs = self.model(**inputs)
                # Use [CLS] token embedding
                embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            
            return embeddings
            
        except Exception as e:
            logger.error(f"Error getting code embeddings: {e}")
            return np.zeros((1, 768))  # Default embedding size
    
    def _assess_vulnerability_type(
        self, 
        chunk: str, 
        chunk_idx: int, 
        file_path: Path, 
        vuln_type: str, 
        pattern_info: Dict[str, Any], 
        embeddings: np.ndarray
    ) -> Optional[AIVulnerabilityAssessment]:
        """Assess if a specific vulnerability type exists in the code chunk."""
        try:
            # Check for keyword presence
            keyword_matches = self._find_keyword_matches(chunk, pattern_info['keywords'])
            
            if not keyword_matches:
                return None
            
            # Check for context patterns
            context_matches = self._find_context_matches(chunk, pattern_info['context_patterns'])
            
            # Calculate confidence score
            confidence = self._calculate_confidence_score(keyword_matches, context_matches, embeddings)
            
            if confidence < 0.3:  # Low confidence threshold
                return None
            
            # Create assessment
            assessment = AIVulnerabilityAssessment(
                vulnerability_type=vuln_type,
                confidence_score=confidence,
                severity_level=pattern_info['severity'],
                context_analysis=pattern_info['description'],
                false_positive_probability=1.0 - confidence,
                exploitation_difficulty=self._assess_exploitation_difficulty(vuln_type, confidence),
                remediation_suggestions=self._get_remediation_suggestions(vuln_type),
                code_context=chunk[:200] + "..." if len(chunk) > 200 else chunk,
                line_numbers=self._estimate_line_numbers(chunk_idx, chunk)
            )
            
            return assessment
            
        except Exception as e:
            logger.error(f"Error assessing vulnerability type {vuln_type}: {e}")
            return None
    
    def _find_keyword_matches(self, code: str, keywords: List[str]) -> List[str]:
        """Find keyword matches in the code."""
        matches = []
        code_lower = code.lower()
        
        for keyword in keywords:
            if keyword.lower() in code_lower:
                matches.append(keyword)
        
        return matches
    
    def _find_context_matches(self, code: str, patterns: List[str]) -> List[str]:
        """Find context pattern matches in the code."""
        matches = []
        
        for pattern in patterns:
            pattern_matches = re.findall(pattern, code, re.IGNORECASE)
            matches.extend(pattern_matches)
        
        return matches
    
    def _calculate_confidence_score(
        self, 
        keyword_matches: List[str], 
        context_matches: List[str], 
        embeddings: np.ndarray
    ) -> float:
        """Calculate confidence score for vulnerability detection."""
        # Base score from keyword matches
        keyword_score = min(len(keyword_matches) * 0.3, 0.6)
        
        # Context score from pattern matches
        context_score = min(len(context_matches) * 0.2, 0.4)
        
        # Embedding-based score (placeholder for now)
        embedding_score = 0.1
        
        # Combine scores
        total_score = keyword_score + context_score + embedding_score
        
        # Normalize to 0-1 range
        return min(total_score, 1.0)
    
    def _assess_exploitation_difficulty(self, vuln_type: str, confidence: float) -> str:
        """Assess exploitation difficulty based on vulnerability type and confidence."""
        if vuln_type in ['sql_injection', 'command_injection']:
            return 'EASY' if confidence > 0.7 else 'MEDIUM'
        elif vuln_type in ['path_traversal', 'xss']:
            return 'MEDIUM' if confidence > 0.6 else 'HARD'
        else:
            return 'MEDIUM'
    
    def _get_remediation_suggestions(self, vuln_type: str) -> List[str]:
        """Get remediation suggestions for the vulnerability type."""
        suggestions = {
            'sql_injection': [
                'Use parameterized queries or prepared statements',
                'Implement input validation and sanitization',
                'Use ORM frameworks that handle SQL injection protection'
            ],
            'command_injection': [
                'Avoid executing system commands with user input',
                'Implement strict input validation',
                'Use built-in APIs instead of shell commands'
            ],
            'path_traversal': [
                'Validate and sanitize file paths',
                'Use canonical paths and path validation',
                'Implement proper file access controls'
            ],
            'xss': [
                'Disable JavaScript for untrusted content',
                'Implement proper input validation and output encoding',
                'Use Content Security Policy (CSP)'
            ],
            'hardcoded_secrets': [
                'Move secrets to secure storage (Android Keystore)',
                'Use environment variables or encrypted configuration',
                'Never commit secrets to source code'
            ],
            'insecure_components': [
                'Review exported components and implement proper permissions',
                'Use intent filters with appropriate access controls',
                'Implement proper authentication and authorization'
            ]
        }
        
        return suggestions.get(vuln_type, ['Review and fix the identified security issue'])
    
    def _estimate_line_numbers(self, chunk_idx: int, chunk: str) -> List[int]:
        """Estimate line numbers for the vulnerability."""
        # This is a simplified estimation
        # In a real implementation, you'd track actual line numbers
        base_line = chunk_idx * 50  # Approximate lines per chunk
        return [base_line + i for i in range(min(5, chunk.count('\n')))]
    
    def _merge_overlapping_assessments(self, assessments: List[AIVulnerabilityAssessment]) -> List[AIVulnerabilityAssessment]:
        """Merge overlapping vulnerability assessments."""
        if not assessments:
            return []
        
        # Sort by confidence score (highest first)
        assessments.sort(key=lambda x: x.confidence_score, reverse=True)
        
        merged = []
        seen_types = set()
        
        for assessment in assessments:
            if assessment.vulnerability_type not in seen_types:
                merged.append(assessment)
                seen_types.add(assessment.vulnerability_type)
        
        return merged
    
    def batch_analyze_files(self, files: List[Tuple[Path, str]]) -> Dict[Path, List[AIVulnerabilityAssessment]]:
        """Analyze multiple files in batch for better performance."""
        results = {}
        
        for file_path, content in files:
            try:
                assessments = self.analyze_code_file(file_path, content)
                results[file_path] = assessments
            except Exception as e:
                logger.error(f"Error analyzing file {file_path}: {e}")
                results[file_path] = []
        
        return results
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about the loaded AI model."""
        return {
            'model_name': self.model_name,
            'device': self.device,
            'ai_available': AI_AVAILABLE,
            'vulnerability_types': list(self.vulnerability_patterns.keys()),
            'context_window': self.context_window
        }

class HybridVulnerabilityDetector:
    """Hybrid detector that combines regex patterns with AI analysis."""
    
    def __init__(self, regex_patterns: Dict, ai_detector: Optional[CodeBERTVulnerabilityDetector] = None):
        """
        Initialize hybrid detector.
        
        Args:
            regex_patterns: Traditional regex patterns
            ai_detector: AI-powered detector (optional)
        """
        self.regex_patterns = regex_patterns
        self.ai_detector = ai_detector
        self.use_ai = ai_detector is not None
        
        logger.info(f"Hybrid Vulnerability Detector initialized (AI: {self.use_ai})")
    
    def detect_vulnerabilities(self, file_path: Path, content: str) -> List[Dict[str, Any]]:
        """
        Detect vulnerabilities using both regex and AI methods.
        
        Args:
            file_path: Path to the file being analyzed
            content: File content as string
            
        Returns:
            List of detected vulnerabilities
        """
        vulnerabilities = []
        
        # 1. Traditional regex detection
        regex_vulns = self._detect_with_regex(file_path, content)
        vulnerabilities.extend(regex_vulns)
        
        # 2. AI-powered detection (if available)
        if self.use_ai:
            try:
                ai_vulns = self._detect_with_ai(file_path, content)
                vulnerabilities.extend(ai_vulns)
            except Exception as e:
                logger.error(f"AI detection failed: {e}")
        
        # 3. Merge and deduplicate results
        vulnerabilities = self._merge_vulnerability_results(vulnerabilities)
        
        return vulnerabilities
    
    def _detect_with_regex(self, file_path: Path, content: str) -> List[Dict[str, Any]]:
        """Detect vulnerabilities using regex patterns."""
        vulnerabilities = []
        
        for category, subcategories in self.regex_patterns.items():
            for subcategory, patterns in subcategories.items():
                for pattern in patterns:
                    matches = re.finditer(pattern, content, re.IGNORECASE)
                    for match in matches:
                        line_num = content[:match.start()].count('\n') + 1
                        
                        vuln = {
                            'file_path': str(file_path),
                            'line_number': line_num,
                            'category': category,
                            'subcategory': subcategory,
                            'pattern': pattern,
                            'matched_text': match.group()[:100],
                            'detection_method': 'regex',
                            'confidence_score': 0.7,  # Default confidence for regex
                            'ai_enhanced': False
                        }
                        
                        vulnerabilities.append(vuln)
        
        return vulnerabilities
    
    def _detect_with_ai(self, file_path: Path, content: str) -> List[Dict[str, Any]]:
        """Detect vulnerabilities using AI analysis."""
        if not self.ai_detector:
            return []
        
        try:
            ai_assessments = self.ai_detector.analyze_code_file(file_path, content)
            
            vulnerabilities = []
            for assessment in ai_assessments:
                vuln = {
                    'file_path': str(file_path),
                    'line_number': assessment.line_numbers[0] if assessment.line_numbers else 0,
                    'category': 'ai_detected',
                    'subcategory': assessment.vulnerability_type,
                    'pattern': 'AI_ANALYSIS',
                    'matched_text': assessment.code_context[:100],
                    'detection_method': 'ai',
                    'confidence_score': assessment.confidence_score,
                    'ai_enhanced': True,
                    'ai_assessment': assessment
                }
                
                vulnerabilities.append(vuln)
            
            return vulnerabilities
            
        except Exception as e:
            logger.error(f"AI detection failed for {file_path}: {e}")
            return []
    
    def _merge_vulnerability_results(self, vulnerabilities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Merge and deduplicate vulnerability results."""
        if not vulnerabilities:
            return []
        
        # Group by file and line number
        grouped = {}
        for vuln in vulnerabilities:
            key = (vuln['file_path'], vuln['line_number'])
            if key not in grouped:
                grouped[key] = []
            grouped[key].append(vuln)
        
        # Merge overlapping vulnerabilities
        merged = []
        for key, vulns in grouped.items():
            if len(vulns) == 1:
                merged.append(vulns[0])
            else:
                # Merge multiple findings for the same location
                merged_vuln = self._merge_overlapping_vulnerabilities(vulns)
                merged.append(merged_vuln)
        
        return merged
    
    def _merge_overlapping_vulnerabilities(self, vulns: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Merge overlapping vulnerabilities at the same location."""
        # Use the highest confidence finding as base
        base_vuln = max(vulns, key=lambda x: x.get('confidence_score', 0))
        
        # Merge additional information
        merged = base_vuln.copy()
        
        # Combine detection methods
        methods = list(set(v['detection_method'] for v in vulns))
        merged['detection_method'] = '+'.join(methods)
        
        # Combine categories and subcategories
        categories = list(set(v['category'] for v in vulns))
        subcategories = list(set(v['subcategory'] for v in vulns))
        
        if len(categories) > 1:
            merged['category'] = 'multiple'
        if len(subcategories) > 1:
            merged['subcategory'] = 'multiple'
        
        # Mark as AI enhanced if any detection used AI
        merged['ai_enhanced'] = any(v.get('ai_enhanced', False) for v in vulns)
        
        return merged
